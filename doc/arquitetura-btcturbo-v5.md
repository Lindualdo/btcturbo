# BTC Turbo v5.0 - Arquitetura TÃ©cnica Completa

## ðŸ“‹ VisÃ£o Geral

Sistema profissional de gestÃ£o para Bitcoin com alavancagem, evoluindo do BTC Turbo v1.0.25 para uma arquitetura moderna focada em **performance**, **manutenibilidade** e **escalabilidade**.

### Stack TecnolÃ³gica
- **Backend**: FastAPI + SQLAlchemy + Redis + Celery
- **Frontend**: React/Next.js + TypeScript + Chart.js
- **Database**: PostgreSQL + Redis Cache
- **Background Jobs**: Celery + Redis Broker
- **Deploy**: Docker Compose + Railway/AWS
- **APIs**: TradingView, AAVE, BigQuery, Notion

---

## ðŸ—ï¸ Nova Arquitetura - Domain-Driven Design

### 1. Estrutura do Projeto

```
btc-turbo-v5/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/                     # ConfiguraÃ§Ãµes globais
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings centralizadas
â”‚   â”‚   â”œâ”€â”€ database.py          # Connection pools
â”‚   â”‚   â”œâ”€â”€ cache.py             # Redis setup
â”‚   â”‚   â””â”€â”€ security.py          # Auth/JWT
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                   # Regras de negÃ³cio
â”‚   â”‚   â”œâ”€â”€ entities/            # Modelos de domÃ­nio
â”‚   â”‚   â”‚   â”œâ”€â”€ market.py        # MarketData, Score
â”‚   â”‚   â”‚   â”œâ”€â”€ position.py      # Position, Trade
â”‚   â”‚   â”‚   â””â”€â”€ backtest.py      # BacktestConfig, Result
â”‚   â”‚   â”œâ”€â”€ repositories/        # Interfaces abstratas
â”‚   â”‚   â””â”€â”€ services/            # LÃ³gica de negÃ³cio pura
â”‚   â”‚       â”œâ”€â”€ score_service.py
â”‚   â”‚       â”œâ”€â”€ position_service.py
â”‚   â”‚       â””â”€â”€ backtest_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/          # ImplementaÃ§Ãµes tÃ©cnicas
â”‚   â”‚   â”œâ”€â”€ repositories/        # ImplementaÃ§Ãµes PostgreSQL
â”‚   â”‚   â”œâ”€â”€ external_apis/       # TradingView, AAVE, etc
â”‚   â”‚   â”œâ”€â”€ cache/              # Redis implementations
â”‚   â”‚   â””â”€â”€ jobs/               # Celery tasks
â”‚   â”‚
â”‚   â”œâ”€â”€ application/             # Use cases e orquestraÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ use_cases/          # Casos de uso especÃ­ficos
â”‚   â”‚   â”œâ”€â”€ commands/           # CQRS Commands
â”‚   â”‚   â”œâ”€â”€ queries/            # CQRS Queries
â”‚   â”‚   â””â”€â”€ handlers/           # Command/Query handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ presentation/            # Camada de apresentaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ api/                # Routers FastAPI
â”‚   â”‚   â”œâ”€â”€ schemas/            # DTOs/Pydantic models
â”‚   â”‚   â””â”€â”€ middleware/         # Middleware customizado
â”‚   â”‚
â”‚   â””â”€â”€ shared/                  # UtilitÃ¡rios compartilhados
â”‚       â”œâ”€â”€ utils/
â”‚       â”œâ”€â”€ constants/
â”‚       â””â”€â”€ exceptions/
â”‚
â”œâ”€â”€ frontend/                    # React/Next.js App
â”‚   â”œâ”€â”€ components/             # Componentes reutilizÃ¡veis
â”‚   â”œâ”€â”€ pages/                  # PÃ¡ginas da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ hooks/                  # Custom React hooks
â”‚   â”œâ”€â”€ services/               # API clients
â”‚   â””â”€â”€ utils/                  # UtilitÃ¡rios frontend
â”‚
â”œâ”€â”€ tests/                      # Testes organizados
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ docker/                     # Docker configs
â”œâ”€â”€ scripts/                    # Scripts de deployment
â””â”€â”€ docs/                       # DocumentaÃ§Ã£o tÃ©cnica
```

### 2. Principais MudanÃ§as da v1.0.25

| Aspecto | v1.0.25 | v5.0 |
|---------|---------|------|
| **Arquitetura** | Monolito com routers | DDD + CQRS |
| **Frontend** | Jinja2 Templates | React/TypeScript SPA |
| **Cache** | Manual em PostgreSQL | Redis distribuÃ­do |
| **Jobs** | SÃ­ncronos nos endpoints | Celery background |
| **Database** | Helpers diretos | Repository Pattern |
| **Testes** | BÃ¡sicos | TDD com 90%+ coverage |
| **Deploy** | Single container | Multi-container compose |

---

## ðŸ“Š EvoluÃ§Ã£o do Sistema de Scores

### v1.0.25 â†’ v5.0 Mapping

#### Indicadores Mantidos
```python
# CICLO (50% â†’ 50%)
MVRV_Z_Score: 25% (mantido)
Realized_Price_Ratio: 20% (mantido) 
Puell_Multiple: 5% (mantido)

# MOMENTUM (20% â†’ 20%)
RSI_Semanal: 8% (mantido)
Funding_Rates: 7% (mantido)
Exchange_Netflow: 3% (reduzido de 5%)
Long_Short_Ratio: 2% (reduzido de 3%)

# TÃ‰CNICO (50% â†’ 30%)
Sistema_EMAs: 20% (mantido)
BBW: 10% (NOVO)

# RISCO (0% â†’ ProteÃ§Ã£o)
Health_Factor: ProteÃ§Ã£o (mantido)
Dist_Liquidacao: ProteÃ§Ã£o (mantido)
```

#### Novos Indicadores v5.0
```python
# Momentum Additions
STH_SOPR: 3%          # Short-Term Holder SOPR
RSI_Monthly: 5%       # Para determinar fase do ciclo

# Technical Additions  
Bollinger_Band_Width: 10%  # Volatilidade comprimida
EMA_Alignment: Integrado ao Sistema EMAs

# Risk Enhancements
Kelly_Criterion: CÃ¡lculo dinÃ¢mico de size
Portfolio_Heat: ExposiÃ§Ã£o total vs ideal
```

---

## ðŸ—„ï¸ Estrutura de Dados Modernizada

### Core Tables (MigraÃ§Ã£o da v1.0.25)

#### market_data (EvoluÃ§Ã£o das tabelas separadas)
```sql
CREATE TABLE market_data (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL UNIQUE,
    
    -- PreÃ§o e Volume
    btc_price DECIMAL(20,2) NOT NULL,
    btc_volume DECIMAL(20,8),
    
    -- Ciclo (migrados de indicadores_ciclo)
    mvrv_z_score DECIMAL(10,4),
    realized_price_ratio DECIMAL(10,4),
    puell_multiple DECIMAL(10,4),
    
    -- Momentum (migrados de indicadores_momentum)
    rsi_weekly DECIMAL(5,2),
    rsi_monthly DECIMAL(5,2),        -- NOVO
    funding_rate_7d DECIMAL(10,6),
    sth_sopr DECIMAL(10,4),          -- NOVO
    exchange_netflow DECIMAL(15,2),
    long_short_ratio DECIMAL(10,4),
    
    -- TÃ©cnico (migrados de indicadores_tecnico)
    ema_17_1w DECIMAL(20,2),
    ema_34_1w DECIMAL(20,2),
    ema_144_1w DECIMAL(20,2),
    ema_305_1w DECIMAL(20,2),
    ema_610_1w DECIMAL(20,2),
    ema_17_1d DECIMAL(20,2),
    ema_34_1d DECIMAL(20,2),
    ema_144_1d DECIMAL(20,2),
    ema_305_1d DECIMAL(20,2),
    ema_610_1d DECIMAL(20,2),
    bbw_percentage DECIMAL(5,2),     -- NOVO
    
    -- Scores Calculados (cache)
    score_cycle DECIMAL(5,2),
    score_momentum DECIMAL(5,2),
    score_technical DECIMAL(5,2),
    score_market DECIMAL(5,2),
    
    -- Metadados
    data_quality JSONB,              -- Quality flags por indicador
    sources JSONB,                   -- Fontes de cada indicador
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Ãndices otimizados
CREATE INDEX idx_market_data_timestamp ON market_data(timestamp DESC);
CREATE INDEX idx_market_data_scores ON market_data(score_market, timestamp);
```

#### positions (Nova tabela para gestÃ£o de posiÃ§Ã£o)
```sql
CREATE TABLE positions (
    id BIGSERIAL PRIMARY KEY,
    user_id UUID NOT NULL,
    
    -- PosiÃ§Ã£o Atual
    btc_amount DECIMAL(20,8) NOT NULL DEFAULT 0,
    entry_price DECIMAL(20,2),
    current_leverage DECIMAL(5,2) DEFAULT 1.0,
    
    -- AAVE Position (migrado de indicadores_risco)
    collateral_usd DECIMAL(20,2),
    debt_usd DECIMAL(20,2),
    health_factor DECIMAL(10,6),
    liquidation_price DECIMAL(20,2),
    
    -- Risk Metrics
    portfolio_value DECIMAL(20,2),
    unrealized_pnl DECIMAL(20,2),
    risk_score DECIMAL(5,2),
    
    -- Strategy State
    strategy_mode VARCHAR(20) DEFAULT 'HOLD', -- HOLD, ACCUMULATE, REDUCE, EXIT
    last_action_date DATE,
    
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### Cache Tables (Redis Backup)
```sql
-- Cache consolidado (evoluÃ§Ã£o de scores_consolidados)
CREATE TABLE score_cache (
    date DATE PRIMARY KEY,
    score_data JSONB NOT NULL,
    alerts JSONB,
    position_data JSONB,
    cached_at TIMESTAMPTZ DEFAULT NOW()
);

-- Performance tracking
CREATE TABLE portfolio_history (
    date DATE PRIMARY KEY,
    portfolio_value DECIMAL(20,2),
    btc_price DECIMAL(20,2),
    total_return_pct DECIMAL(10,4),
    leverage DECIMAL(5,2),
    drawdown_pct DECIMAL(10,4)
);
```

### Novas Tabelas v5.0

#### backtest_results
```sql
CREATE TABLE backtest_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    config JSONB NOT NULL,           -- ParÃ¢metros do backtest
    
    -- PerÃ­odo
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    initial_capital DECIMAL(20,2),
    
    -- Resultados
    final_value DECIMAL(20,2),
    total_return DECIMAL(10,4),
    sharpe_ratio DECIMAL(10,4),
    max_drawdown DECIMAL(10,4),
    win_rate DECIMAL(5,2),
    total_trades INTEGER,
    
    -- ComparaÃ§Ã£o
    btc_hold_return DECIMAL(10,4),
    outperformance DECIMAL(10,4),
    
    status VARCHAR(20) DEFAULT 'RUNNING',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ
);

CREATE TABLE backtest_trades (
    id BIGSERIAL PRIMARY KEY,
    backtest_id UUID REFERENCES backtest_runs(id),
    timestamp TIMESTAMPTZ NOT NULL,
    
    -- Contexto de DecisÃ£o
    action VARCHAR(20) NOT NULL,     -- ENTER, ADD, REDUCE, EXIT
    trigger_reason TEXT,
    score_market DECIMAL(5,2),
    mvrv_value DECIMAL(10,4),
    ema_distance_pct DECIMAL(10,2),
    rsi_daily DECIMAL(5,2),
    
    -- ExecuÃ§Ã£o
    price DECIMAL(20,2) NOT NULL,
    size_pct DECIMAL(5,2),
    leverage_before DECIMAL(5,2),
    leverage_after DECIMAL(5,2),
    
    -- Resultado
    pnl_usd DECIMAL(20,2),
    pnl_pct DECIMAL(10,4),
    portfolio_value DECIMAL(20,2)
);
```

---

## ðŸš€ Services Architecture

### 1. Domain Services (LÃ³gica de NegÃ³cio Pura)

#### ScoreService
```python
@dataclass
class MarketScores:
    cycle: float
    momentum: float  
    technical: float
    market: float
    confidence: float

class ScoreService:
    def calculate_market_score(self, data: MarketData) -> MarketScores:
        """Calcula scores seguindo especificaÃ§Ã£o v5.0"""
        
        cycle_score = self._calculate_cycle_score(
            mvrv=data.mvrv_z_score,
            realized_ratio=data.realized_price_ratio,
            puell=data.puell_multiple
        )
        
        momentum_score = self._calculate_momentum_score(
            rsi_weekly=data.rsi_weekly,
            rsi_monthly=data.rsi_monthly,
            funding=data.funding_rate_7d,
            sth_sopr=data.sth_sopr,
            netflow=data.exchange_netflow,
            ls_ratio=data.long_short_ratio
        )
        
        technical_score = self._calculate_technical_score(
            emas=data.ema_data,
            price=data.btc_price,
            bbw=data.bbw_percentage
        )
        
        market_score = (
            cycle_score * 0.50 +
            momentum_score * 0.20 + 
            technical_score * 0.30
        )
        
        return MarketScores(
            cycle=cycle_score,
            momentum=momentum_score,
            technical=technical_score,
            market=market_score,
            confidence=self._calculate_confidence(data)
        )
```

#### PositionService
```python
@dataclass
class PositionDecision:
    action: str  # HOLD, ADD, REDUCE, EXIT
    size_percent: float
    target_leverage: float
    reason: str
    urgency: str  # LOW, MEDIUM, HIGH, CRITICAL

class PositionService:
    def evaluate_position(
        self, 
        scores: MarketScores,
        current_position: Position,
        market_data: MarketData
    ) -> PositionDecision:
        """Implementa matriz de decisÃ£o v5.0"""
        
        # Circuit Breakers (Prioridade 1)
        if current_position.risk_score < 30:
            return PositionDecision(
                action="EXIT",
                size_percent=100,
                target_leverage=0,
                reason="Risk score crÃ­tico",
                urgency="CRITICAL"
            )
        
        # Score de mercado baixo
        if scores.market < 40:
            return PositionDecision(
                action="REDUCE", 
                size_percent=50,
                target_leverage=current_position.leverage * 0.5,
                reason="Score mercado baixo",
                urgency="HIGH"
            )
        
        # ExecuÃ§Ã£o tÃ¡tica (EMA 144 + RSI)
        return self._tactical_execution(
            ema_distance=market_data.ema_144_distance,
            rsi_daily=market_data.rsi_daily,
            current_position=current_position
        )
```

### 2. Application Layer (Use Cases)

#### AnalyzeMarketUseCase
```python
class AnalyzeMarketUseCase:
    def __init__(
        self,
        market_repo: MarketDataRepository,
        score_service: ScoreService,
        cache_service: CacheService
    ):
        self.market_repo = market_repo
        self.score_service = score_service
        self.cache = cache_service
    
    async def execute(self, force_refresh: bool = False) -> MarketAnalysis:
        # 1. Check cache
        if not force_refresh:
            cached = await self.cache.get_market_analysis()
            if cached and cached.is_fresh():
                return cached
        
        # 2. Get latest data
        market_data = await self.market_repo.get_latest()
        
        # 3. Calculate scores
        scores = self.score_service.calculate_market_score(market_data)
        
        # 4. Generate alerts
        alerts = self.alert_service.generate_alerts(scores, market_data)
        
        # 5. Cache result
        result = MarketAnalysis(
            scores=scores,
            alerts=alerts,
            timestamp=datetime.utcnow()
        )
        await self.cache.set_market_analysis(result)
        
        return result
```

### 3. Infrastructure Layer

#### Repository Implementations
```python
class PostgreSQLMarketDataRepository(MarketDataRepository):
    def __init__(self, db_session: AsyncSession):
        self.db = db_session
    
    async def get_latest(self) -> MarketData:
        query = select(MarketDataModel).order_by(
            MarketDataModel.timestamp.desc()
        ).limit(1)
        
        result = await self.db.execute(query)
        model = result.scalar_one()
        
        return MarketData.from_orm(model)
    
    async def get_historical(
        self, 
        start: datetime, 
        end: datetime
    ) -> List[MarketData]:
        query = select(MarketDataModel).where(
            MarketDataModel.timestamp.between(start, end)
        ).order_by(MarketDataModel.timestamp)
        
        result = await self.db.execute(query)
        models = result.scalars().all()
        
        return [MarketData.from_orm(model) for model in models]
```

---

## âš¡ Background Jobs com Celery

### 1. Data Collection Jobs
```python
# app/infrastructure/jobs/collectors.py

@celery_app.task(bind=True, max_retries=3)
def collect_market_data(self):
    """Coleta dados a cada 5 minutos"""
    try:
        # TradingView EMAs
        ema_data = TradingViewAPI().get_emas()
        
        # BigQuery MVRV
        mvrv_data = BigQueryAPI().get_mvrv()
        
        # Notion indicators
        notion_data = NotionAPI().get_indicators()
        
        # Consolidate and save
        market_data = MarketData.combine(ema_data, mvrv_data, notion_data)
        await market_repo.save(market_data)
        
        # Trigger score calculation
        calculate_scores.delay(market_data.timestamp)
        
    except Exception as exc:
        self.retry(countdown=60, exc=exc)

@celery_app.task
def collect_position_data():
    """Coleta dados da posiÃ§Ã£o AAVE a cada 1 minuto"""
    position_data = AAVEWebAPI().get_position()
    await position_repo.update_current(position_data)

@celery_app.task
def calculate_scores(timestamp: datetime):
    """Calcula scores para timestamp especÃ­fico"""
    market_data = await market_repo.get_by_timestamp(timestamp)
    scores = score_service.calculate_market_score(market_data)
    
    # Update cache
    await cache.set_scores(timestamp, scores)
    
    # Check alerts
    check_alerts.delay(timestamp)
```

### 2. Celery Configuration
```python
# app/core/celery_config.py

celery_app = Celery(
    "btc_turbo",
    broker="redis://redis:6379/0",
    backend="redis://redis:6379/0"
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    
    # Scheduled tasks
    beat_schedule={
        "collect-market-data": {
            "task": "collect_market_data",
            "schedule": 300.0,  # 5 minutes
        },
        "collect-position-data": {
            "task": "collect_position_data", 
            "schedule": 60.0,   # 1 minute
        },
        "daily-report": {
            "task": "generate_daily_report",
            "schedule": crontab(hour=8, minute=0),
        }
    }
)
```

---

## ðŸŽ¯ API Modernizada

### 1. Estrutura de Endpoints

```python
# app/presentation/api/v5/

# Market Analysis
GET    /api/v5/market/analysis      # AnÃ¡lise completa
GET    /api/v5/market/scores        # Scores atuais
GET    /api/v5/market/history       # HistÃ³rico scores

# Position Management  
GET    /api/v5/position/current     # PosiÃ§Ã£o atual
POST   /api/v5/position/evaluate   # Avaliar decisÃ£o
POST   /api/v5/position/execute    # Executar aÃ§Ã£o

# Backtest
POST   /api/v5/backtest/run         # Novo backtest
GET    /api/v5/backtest/{id}        # Resultados
GET    /api/v5/backtest/{id}/trades # Trades detalhados

# Alerts
GET    /api/v5/alerts/active        # Alertas ativos
POST   /api/v5/alerts/acknowledge   # Marcar como lido
PUT    /api/v5/alerts/config        # Configurar alertas

# Real-time
WS     /api/v5/ws/scores           # WebSocket scores
WS     /api/v5/ws/alerts           # WebSocket alertas
```

### 2. Response Schemas Padronizados

```python
# app/presentation/schemas/responses.py

class MarketAnalysisResponse(BaseModel):
    timestamp: datetime
    scores: ScoresData
    decision: PositionDecision
    alerts: List[Alert]
    metadata: AnalysisMetadata

class ScoresData(BaseModel):
    market: float = Field(..., ge=0, le=100)
    cycle: float = Field(..., ge=0, le=100)  
    momentum: float = Field(..., ge=0, le=100)
    technical: float = Field(..., ge=0, le=100)
    confidence: float = Field(..., ge=0, le=100)
    
    class Config:
        schema_extra = {
            "example": {
                "market": 67.5,
                "cycle": 55.0,
                "momentum": 72.0,
                "technical": 75.0,
                "confidence": 85.0
            }
        }
```

---

## ðŸ”„ Migration Strategy (v1.0.25 â†’ v5.0)

### Fase 1: Data Migration
```sql
-- Script de migraÃ§Ã£o de dados
INSERT INTO market_data (
    timestamp, btc_price,
    mvrv_z_score, realized_price_ratio, puell_multiple,
    rsi_weekly, funding_rate_7d, exchange_netflow, long_short_ratio,
    ema_17_1w, ema_34_1w, ema_144_1w, ema_305_1w, ema_610_1w,
    ema_17_1d, ema_34_1d, ema_144_1d, ema_305_1d, ema_610_1d
)
SELECT 
    COALESCE(c.timestamp, m.timestamp, r.timestamp, t.timestamp) as timestamp,
    t.btc_price_current,
    
    -- Ciclo
    c.mvrv_z_score, c.realized_ratio, c.puell_multiple,
    
    -- Momentum  
    m.rsi_semanal, m.funding_rates, m.exchange_netflow, m.long_short_ratio,
    
    -- TÃ©cnico
    t.ema_17_1w, t.ema_34_1w, t.ema_144_1w, t.ema_305_1w, t.ema_610_1w,
    t.ema_17_1d, t.ema_34_1d, t.ema_144_1d, t.ema_305_1d, t.ema_610_1d

FROM indicadores_ciclo c
FULL OUTER JOIN indicadores_momentum m ON DATE(c.timestamp) = DATE(m.timestamp)
FULL OUTER JOIN indicadores_risco r ON DATE(c.timestamp) = DATE(r.timestamp)  
FULL OUTER JOIN indicadores_tecnico t ON DATE(c.timestamp) = DATE(t.timestamp)
ORDER BY timestamp;
```

### Fase 2: Code Migration
1. **Week 1**: Setup nova estrutura + repositories
2. **Week 2**: Migrar services + use cases  
3. **Week 3**: Nova API + endpoints
4. **Week 4**: Frontend React + testes

### Fase 3: Deployment
1. **Blue-Green**: Deploy v5.0 paralelo Ã  v1.0.25
2. **Data Sync**: Celery jobs comeÃ§am popular nova base
3. **Validation**: Comparar outputs por 1 semana
4. **Switch**: Redirecionar DNS para v5.0
5. **Cleanup**: Desativar v1.0.25 apÃ³s validaÃ§Ã£o

---

## ðŸ“Š Performance Improvements

### Database Optimizations
```sql
-- Ãndices estratÃ©gicos
CREATE INDEX CONCURRENTLY idx_market_data_latest 
ON market_data(timestamp DESC) WHERE timestamp > NOW() - INTERVAL '7 days';

CREATE INDEX CONCURRENTLY idx_scores_analysis
ON market_data(score_market, score_cycle, score_momentum, score_technical)
WHERE timestamp > NOW() - INTERVAL '30 days';

-- Particionamento por mÃªs
CREATE TABLE market_data (
    -- columns
) PARTITION BY RANGE (timestamp);

CREATE TABLE market_data_2025_01 PARTITION OF market_data
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

### Redis Caching Strategy
```python
# Cache layers
CACHE_LAYERS = {
    "L1_SCORES": 60,          # 1 minuto - scores atuais
    "L2_ANALYSIS": 300,       # 5 minutos - anÃ¡lise completa  
    "L3_HISTORICAL": 3600,    # 1 hora - dados histÃ³ricos
    "L4_BACKTEST": 86400      # 1 dia - resultados backtest
}

# Cache warming
@celery_app.task
def warm_cache():
    """Preaquece cache com dados mais usados"""
    await cache.set_scores(get_latest_scores())
    await cache.set_market_analysis(get_latest_analysis())
    await cache.set_position(get_current_position())
```

### Connection Pooling
```python
# app/core/database.py
from sqlalchemy.pool import QueuePool

engine = create_async_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

---

## ðŸ§ª Testing Strategy

### Test Structure
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ test_score_service.py
â”‚   â”‚   â””â”€â”€ test_position_service.py
â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â””â”€â”€ test_use_cases.py
â”‚   â””â”€â”€ infrastructure/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_repositories.py
â”‚   â”œâ”€â”€ test_external_apis.py
â”‚   â””â”€â”€ test_celery_tasks.py
â””â”€â”€ e2e/
    â”œâ”€â”€ test_api_endpoints.py
    â””â”€â”€ test_full_workflow.py
```

### Coverage Requirements
- **Unit Tests**: 95%+ domain services
- **Integration**: 85%+ repositories/APIs  
- **E2E**: Fluxos crÃ­ticos 100%

---

## ðŸš€ Deployment Architecture

### Docker Compose Production
```yaml
version: '3.8'
services:
  api:
    build: ./app
    depends_on: [postgres, redis]
    environment:
      - DATABASE_URL=postgresql://...
      - REDIS_URL=redis://redis:6379
    
  worker:
    build: ./app
    command: celery -A app.core.celery_app worker -l info
    depends_on: [postgres, redis]
    
  scheduler:
    build: ./app 
    command: celery -A app.core.celery_app beat -l info
    depends_on: [redis]
    
  frontend:
    build: ./frontend
    depends_on: [api]
    
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: btc_turbo_v5
    volumes:
      - postgres_data:/var/lib/postgresql/data
      
  redis:
    image: redis:7
    volumes:
      - redis_data:/data
```

---

## ðŸŽ¯ Roadmap v5.0

### Sprint 1-2: Foundation (4 weeks)
- [ ] Setup nova estrutura DDD
- [ ] Implementar repositories + unit tests
- [ ] Migration scripts de dados  
- [ ] Core services (Score, Position)

### Sprint 3-4: Application Layer (4 weeks)
- [ ] Use cases + handlers
- [ ] Celery jobs configuration
- [ ] Cache strategy implementation
- [ ] API endpoints bÃ¡sicos

### Sprint 5-6: Frontend + Integration (4 weeks)  
- [ ] React app setup + components
- [ ] WebSocket real-time updates
- [ ] Integration tests completos
- [ ] Performance optimization

### Sprint 7-8: Production + Backtest (4 weeks)
- [ ] Deploy pipeline + monitoring
- [ ] Backtest engine completo
- [ ] Load testing + ajustes
- [ ] Go-live + validation

### Sprint 9-12: Advanced Features (8 weeks)
- [ ] Machine learning overlay
- [ ] Advanced risk management
- [ ] Mobile app (React Native)
- [ ] Multi-user support

---

## ðŸ“ˆ Expected Improvements

### Performance Gains
- **API Response**: 500ms â†’ 50ms (cache + async)
- **Data Processing**: 2min â†’ 30sec (Celery background)
- **Dashboard Load**: 3sec â†’ 800ms (React SPA)
- **Concurrent Users**: 10 â†’ 100+ (connection pooling)

### Development Productivity  
- **Test Coverage**: 30% â†’ 90%+
- **Deployment Time**: 15min â†’ 2min
- **Feature Development**: 2 semanas â†’ 1 semana
- **Bug Detection**: ProduÃ§Ã£o â†’ Development (TDD)

### Business Value
- **System Reliability**: 95% â†’ 99.5% uptime
- **Decision Latency**: 5min â†’ 30sec
- **Strategy Confidence**: Qualitativo â†’ Quantitativo (backtest)
- **Risk Management**: Manual â†’ Automatizado

---

## ðŸŽ¯ Success Metrics

### Technical KPIs
- [ ] API response time P95 < 100ms
- [ ] Background job completion rate > 99%
- [ ] Test coverage > 90%
- [ ] Zero data loss during migration
- [ ] System uptime > 99.5%

### Business KPIs  
- [ ] Decision accuracy vs manual > 95%
- [ ] Time to market